# 毕业设计架构总结：基于张量计算的大型稀疏矩阵求解系统

## 1. 核心理论基础

### 1.1 算法选择：Multifrontal Method (多波前法)

- **适用对象**：稀疏非对称矩阵 $A$。
- **结构预处理**：基于 $A+A^T$ 的非零元模式构建 **Elimination Tree (消解树)**。
  - *注意*：仅利用 $A+A^T$ 确定计算依赖结构和内存分配，数值填充依然来源于原始矩阵 $A$。
- **计算核心**：将稀疏分解转化为树节点上的稠密矩阵运算。
  - 每个节点处理一个**波前矩阵 (Frontal Matrix)**。
  - 波前矩阵 $F = \begin{pmatrix} F_{11} & F_{12} \\ F_{21} & F_{22} \end{pmatrix}$。
  - $F_{11}, F_{12}, F_{21}$ 对应当前节点的主元部分（最终形成 LU 因子）。
  - $F_{22}$ 是 **Schur Complement (更新矩阵/Update Matrix)**，用于传递给父节点。

### 1.2 性能关键：Supernode (超节点)

- **定义**：将消解树中具有相同或包含非零元结构的连续列合并。
- **目的**：将标量操作转化为**矩阵-矩阵乘法 (GEMM)**，以适配张量计算单元（Tensor Core），利用 BLAS-3 级性能。
- **策略**：软件端负责识别超节点，并将其切分（Splitting）为适配硬件缓存的块大小（Hardware Tile Size）。

------

## 2. 软硬件协同架构

### 2.1 任务划分

- **Host (软件层)**：
  - 符号分析（Symbolic Analysis）：构建消解树，计算 Fill-in。
  - 超节点管理：限制最大超节点尺寸（如 $256 \times 256$）。
  - 内存布局：预分配所有节点的内存空间（包含因子存储区和波前矩阵工作区）。
  - 任务调度：生成 `Node_Task` 描述符，按后序遍历（Post-order）顺序发送给硬件。
- **FPGA (硬件层)**：
  - 执行具体的数值计算。
  - 处理 $256 \times 256$ 稠密块的 Blocked LU 分解。
  - 执行 Extend-Add（数据组装）操作。

### 2.2 硬件计算流程：Right-Looking Blocked LU

针对一个 $256 \times 256$ 的任务块，硬件采用分块流水线（假设 Block Size $b=32$）：

1. **Panel Factorization (SFU)**：处理当前对角线 $32 \times 32$ 块的分解及列/行更新。
2. **Trailing Update (Tensor Core)**：利用算好的 Panel，更新右下角剩余的大矩阵 ($A = A - L_{panel} \times U_{panel}$)。
3. **迭代**：循环执行直到完成所有 Block。

------

## 3. 关键硬件模块设计

### 3.1 HPU (Hazard/Pivot Unit) - 数值稳定性卫士

- **功能**：解决主元选取（Pivoting）问题，防止数值不稳定。
- **搜索范围**：当前处理列在整个超节点内的剩余部分（Vertical Scope, e.g., Row $k$ to 255）。
- **算法**：**Partial Pivoting (部分主元法)**。
- **实现模式**：**竞标赛树 (Tournament Tree)**。寻找全局最大值，而非阈值判定。
- **流水线策略**：**Lookahead (前瞻模式)**。
  - *拒绝*：静态预先重排（数学上不安全，会导致后续数据变 0 引发错误）。
  - *采用*：当 Tensor Core 正在更新 Panel $K+2 \dots N$ 时，HPU 提前并行处理 Panel $K+1$ 的主元搜索。

### 3.2 ATU (Address Translation Unit) - 零拷贝存储路由

- **功能**：解决 Pivot 带来的数据搬运开销。
- **机制**：**Local Indirection (局部间接寻址)**。
  - 维护一个映射表 $P$: `Logical_Row -> Physical_BRAM_Addr`。
  - HPU 决定交换行时，仅修改表 $P$ 中的寄存器值。
  - 后续计算单元访问时，通过 ATU 转换地址，无需物理搬运 $1\text{KB}$ 的行数据。
- **作用域**：仅限当前节点（$256 \times 256$）内部。

### 3.3 Scatter Engine - 节点间数据组装

- **功能**：执行 Extend-Add 操作，将子节点的 Update Matrix 加到父节点。
- **机制**：**Relative Mapping Table (相对映射表)**。
  - *拒绝*：在硬件中使用“全局行号”进行搜索（导致极大的 Latency）。
  - *采用*：软件预计算映射表 `Map[i] = j`（子节点第 $i$ 行 = 父节点第 $j$ 行）。
  - 硬件 DMA 根据 `Base_Addr + Map[i]` 直接计算目标物理地址进行累加。

------

## 4. 数据结构与存储协议

### 4.1 任务描述符 (Node_Task)

软件写入 DDR，硬件读取以执行任务：

C

```
struct Node_Task {
    uint32_t total_dim;       // 当前波前矩阵维数 (e.g., 256)
    uint32_t pivot_dim;       // 主元块大小 (e.g., 256 or less)
    
    // --- 输入/工作区 ---
    uint64_t data_addr;       // 波前矩阵在 DDR 的基地址 (包含组装好的数据)
    
    // --- 组装映射 (用于 Update Matrix) ---
    uint32_t parent_base_addr;// 父节点基地址
    uint64_t map_table_addr;  // 映射表指针 (Scatter Map)
    
    // --- 结果输出 (用于 LU Factors) ---
    uint64_t l_factor_addr;   // L 因子永久存储地址
    uint64_t u_factor_addr;   // U 因子永久存储地址
};
```

### 4.2 结果存储策略 (分流)

- **Update Matrix ($S$)**：
  - **性质**：中间变量。
  - **操作**：通过 Scatter Engine **累加** 到父节点的 DDR 空间。
  - **后续**：被父节点作为输入读取，随后废弃。
- **LU Factors ($L, U$)**：
  - **性质**：最终求解结果。
  - **操作**：以稠密块的形式 **写入** DDR 的 Factor 区域。
  - **形态**：内存中没有单一的大稀疏矩阵，只有离散分布的稠密 L/U 块（形成一棵树）。

------

## 5. 系统整体工作流 (Pipeline)

1. **预处理 (SW)**：构建树，分配内存，生成 `Node_Task` 和 `Map_Table`。
2. **加载 (HW)**：DMA 读取 Task 和 Data 到片上 Buffer。
3. **计算 (HW)**：
   - **Init**：重置 ATU。
   - **Loop**：
     - **HPU** (Lookahead)：并行搜索下一个 Panel 的主元，更新 ATU。
     - **SFU**：计算当前 Panel 的因子 ($L_{panel}, U_{panel}$)。
     - **Tensor Core**：大规模 GEMM 更新剩余矩阵。
4. **写回 (HW)**：
   - **Save Factors**：将算好的 $L, U$ 块 Burst Write 到 DDR (`l/u_factor_addr`)。
   - **Scatter Update**：将剩余的 Schur Complement 根据 `map_table` 累加到父节点 DDR (`parent_base_addr`)。

------

## 6. 创新点总结 (用于论文/答辩)

1. **ATU 零拷贝主元交换**：解决了传统 LU 分解中数据频繁移动导致的带宽瓶颈。
2. **HPU 前瞻流水线 (Lookahead)**：在保证数值稳定性（Partial Pivoting）的前提下，通过并行掩盖了搜索延迟。
3. **异构计算核心**：SFU 处理标量瓶颈，Tensor Core 处理矩阵吞吐，实现了算力最大化。
4. **基于映射表的 Extend-Add**：避免了硬件层面的全局索引搜索，极大提升了稀疏矩阵组装效率。